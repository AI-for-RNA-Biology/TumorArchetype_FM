{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a62ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Utility function to properly load parquet with spot names\n",
    "def load_parquet_with_spots(file_path, n_rows=None):\n",
    "    \"\"\"\n",
    "    Load parquet file and properly set spot names as index\n",
    "    \"\"\"\n",
    "    if n_rows:\n",
    "        # Read limited rows\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        first_batch = next(parquet_file.iter_batches(batch_size=n_rows))\n",
    "        df = first_batch.to_pandas().head(n_rows)\n",
    "    else:\n",
    "        # Read full file (use with caution!)\n",
    "        df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Set spot_id as index if it exists\n",
    "    if 'spot_id' in df.columns:\n",
    "        df = df.set_index('spot_id')\n",
    "        print(f\"✅ Set spot_id as index. Sample spot names: {df.index[:3].tolist()}\")\n",
    "    else:\n",
    "        print(f\"❌ No spot_id column found. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5a2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRULY memory-efficient parquet reading with pyarrow\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c391fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with your file\n",
    "file_path = \"/storage/research/dbmr_luisierlab/temp/lfournier/repositories/TumorArchetype-FM/results/molecular/TNBC_processed/TNBC_filtered_counts.parquet\"\n",
    "annotation = pd.read_csv(\"/storage/research/dbmr_luisierlab/temp/lfournier/repositories/TumorArchetype-FM/results/compute_patches/TNBC/spots_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b980573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient tissue-specific spot loading\n",
    "def load_parquet_by_tissue_chunks(file_path, annotation, target_tissues, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Load only spots from specific tissue types using chunked processing.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to parquet file\n",
    "    - annotation: DataFrame with spot annotations (index = spot names, 'label' column = tissue type)\n",
    "    - target_tissues: List of tissue types to load (e.g., ['adipose tissue'])\n",
    "    - chunk_size: Number of rows to process per chunk\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with only spots from target tissues\n",
    "    \"\"\"\n",
    "    # Get target spot names\n",
    "    target_spots = set(annotation[annotation['label'].isin(target_tissues)].index)\n",
    "    print(f\"Target tissues: {target_tissues}\")\n",
    "    print(f\"Found {len(target_spots)} spots in target tissues\")\n",
    "    \n",
    "    if len(target_spots) == 0:\n",
    "        print(\"❌ No spots found for target tissues!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process file in chunks\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    matching_chunks = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    print(f\"Processing parquet file in chunks of {chunk_size}...\")\n",
    "    \n",
    "    for i, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):\n",
    "        chunk_df = batch.to_pandas()\n",
    "        chunk_df['spot_id'] = chunk_df['spot_id'].apply(lambda x: x.split('parquet.')[1].replace('X', 'spot'))\n",
    "        total_processed += len(chunk_df)\n",
    "        \n",
    "        # Check if spot_id column exists\n",
    "        if 'spot_id' in chunk_df.columns:\n",
    "            # Filter for target spots\n",
    "            matching_spots = chunk_df[chunk_df['spot_id'].isin(target_spots)]\n",
    "            if len(matching_spots) > 0:\n",
    "                matching_spots = matching_spots.set_index('spot_id')\n",
    "                \n",
    "                matching_chunks.append(matching_spots)\n",
    "                print(f\"  Chunk {i+1}: Found {len(matching_spots)} matching spots\")\n",
    "        \n",
    "        # Progress update every 10 chunks\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processed {total_processed} rows...\")\n",
    "    \n",
    "    # Combine all matching chunks\n",
    "    if matching_chunks:\n",
    "        result = pd.concat(matching_chunks, axis=0)\n",
    "        print(f\"✅ Successfully loaded {len(result)} spots from {len(matching_chunks)} chunks\")\n",
    "        \n",
    "        # Show tissue distribution\n",
    "        result_annotation = annotation.loc[result.index]\n",
    "        tissue_counts = result_annotation['label'].value_counts()\n",
    "        print(\"Tissue distribution in result:\")\n",
    "        for tissue, count in tissue_counts.items():\n",
    "            print(f\"  {tissue}: {count} spots\")\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print(\"❌ No matching spots found in any chunk!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def load_adipose_tissue_spots(file_path, annotation, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Convenience function to load only adipose tissue spots.\n",
    "    \"\"\"\n",
    "    return load_parquet_by_tissue_chunks(file_path, annotation, ['adipose tissue'], chunk_size)\n",
    "\n",
    "\n",
    "def load_multiple_tissues(file_path, annotation, tissue_list, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Convenience function to load spots from multiple tissue types.\n",
    "    \n",
    "    Parameters:\n",
    "    - tissue_list: List of tissue types (e.g., ['adipose tissue', 'connective tissue'])\n",
    "    \"\"\"\n",
    "    return load_parquet_by_tissue_chunks(file_path, annotation, tissue_list, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e2fec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING MULTIPLE TISSUE TYPES ===\n",
      "Available tissue types:\n",
      "undetermined         53782\n",
      "connective tissue    30330\n",
      "invasive cancer      12106\n",
      "adipose tissue        3716\n",
      "immune infiltrate     1873\n",
      "cancer in situ          96\n",
      "breast glands           51\n",
      "Name: label, dtype: int64\n",
      "Target tissues: ['adipose tissue', 'connective tissue', 'invasive cancer', 'immune infiltrate', 'breast glands', 'cancer in situ']\n",
      "Found 48172 spots in target tissues\n",
      "Processing parquet file in chunks of 500...\n",
      "  Chunk 1: Found 280 matching spots\n",
      "  Chunk 2: Found 218 matching spots\n",
      "  Chunk 3: Found 342 matching spots\n",
      "  Chunk 4: Found 148 matching spots\n",
      "  Chunk 5: Found 172 matching spots\n",
      "  Chunk 6: Found 300 matching spots\n",
      "  Chunk 7: Found 320 matching spots\n",
      "  Chunk 8: Found 286 matching spots\n",
      "  Chunk 9: Found 336 matching spots\n",
      "  Chunk 10: Found 326 matching spots\n",
      "  Processed 5000 rows...\n",
      "  Chunk 11: Found 323 matching spots\n",
      "  Chunk 12: Found 316 matching spots\n",
      "  Chunk 13: Found 157 matching spots\n",
      "  Chunk 14: Found 132 matching spots\n",
      "  Chunk 15: Found 157 matching spots\n",
      "  Chunk 16: Found 252 matching spots\n",
      "  Chunk 17: Found 188 matching spots\n",
      "  Chunk 18: Found 186 matching spots\n",
      "  Chunk 19: Found 277 matching spots\n",
      "  Chunk 20: Found 313 matching spots\n",
      "  Processed 10000 rows...\n",
      "  Chunk 21: Found 293 matching spots\n",
      "  Chunk 22: Found 393 matching spots\n",
      "  Chunk 23: Found 343 matching spots\n",
      "  Chunk 24: Found 243 matching spots\n",
      "  Chunk 25: Found 198 matching spots\n",
      "  Chunk 26: Found 161 matching spots\n",
      "  Chunk 27: Found 229 matching spots\n",
      "  Chunk 28: Found 276 matching spots\n",
      "  Chunk 29: Found 318 matching spots\n",
      "  Chunk 30: Found 304 matching spots\n",
      "  Processed 15000 rows...\n",
      "  Chunk 31: Found 334 matching spots\n",
      "  Chunk 32: Found 262 matching spots\n",
      "  Chunk 33: Found 204 matching spots\n",
      "  Chunk 34: Found 216 matching spots\n",
      "  Chunk 35: Found 300 matching spots\n",
      "  Chunk 36: Found 294 matching spots\n",
      "  Chunk 37: Found 383 matching spots\n",
      "  Chunk 38: Found 284 matching spots\n",
      "  Chunk 39: Found 88 matching spots\n",
      "  Chunk 40: Found 100 matching spots\n",
      "  Processed 20000 rows...\n",
      "  Chunk 41: Found 206 matching spots\n",
      "  Chunk 42: Found 276 matching spots\n",
      "  Chunk 43: Found 304 matching spots\n",
      "  Chunk 44: Found 287 matching spots\n",
      "  Chunk 45: Found 151 matching spots\n",
      "  Chunk 46: Found 213 matching spots\n",
      "  Chunk 47: Found 197 matching spots\n",
      "  Chunk 48: Found 303 matching spots\n",
      "  Chunk 49: Found 261 matching spots\n",
      "  Chunk 50: Found 251 matching spots\n",
      "  Processed 25000 rows...\n",
      "  Chunk 51: Found 289 matching spots\n",
      "  Chunk 52: Found 263 matching spots\n",
      "  Chunk 53: Found 406 matching spots\n",
      "  Chunk 54: Found 218 matching spots\n",
      "  Chunk 55: Found 148 matching spots\n",
      "  Chunk 56: Found 379 matching spots\n",
      "  Chunk 57: Found 275 matching spots\n",
      "  Chunk 58: Found 191 matching spots\n",
      "  Chunk 59: Found 126 matching spots\n",
      "  Chunk 60: Found 266 matching spots\n",
      "  Processed 30000 rows...\n",
      "  Chunk 61: Found 169 matching spots\n",
      "  Chunk 62: Found 80 matching spots\n",
      "  Chunk 63: Found 53 matching spots\n",
      "  Chunk 64: Found 178 matching spots\n",
      "  Chunk 65: Found 235 matching spots\n",
      "  Chunk 66: Found 205 matching spots\n",
      "  Chunk 67: Found 341 matching spots\n",
      "  Chunk 68: Found 400 matching spots\n",
      "  Chunk 69: Found 307 matching spots\n",
      "  Chunk 70: Found 112 matching spots\n",
      "  Processed 35000 rows...\n",
      "  Chunk 71: Found 108 matching spots\n",
      "  Chunk 72: Found 179 matching spots\n",
      "  Chunk 73: Found 207 matching spots\n",
      "  Chunk 74: Found 213 matching spots\n",
      "  Chunk 75: Found 198 matching spots\n",
      "  Chunk 76: Found 237 matching spots\n",
      "  Chunk 77: Found 334 matching spots\n",
      "  Chunk 78: Found 259 matching spots\n",
      "  Chunk 79: Found 280 matching spots\n",
      "  Chunk 80: Found 234 matching spots\n",
      "  Processed 40000 rows...\n",
      "  Chunk 81: Found 238 matching spots\n",
      "  Chunk 82: Found 256 matching spots\n",
      "  Chunk 83: Found 257 matching spots\n",
      "  Chunk 84: Found 233 matching spots\n",
      "  Chunk 85: Found 223 matching spots\n",
      "  Chunk 86: Found 319 matching spots\n",
      "  Chunk 87: Found 232 matching spots\n",
      "  Chunk 88: Found 221 matching spots\n",
      "  Chunk 89: Found 103 matching spots\n",
      "  Chunk 90: Found 148 matching spots\n",
      "  Processed 45000 rows...\n",
      "  Chunk 91: Found 195 matching spots\n",
      "  Chunk 92: Found 291 matching spots\n",
      "  Chunk 93: Found 340 matching spots\n",
      "  Chunk 94: Found 341 matching spots\n",
      "  Chunk 95: Found 307 matching spots\n",
      "  Chunk 96: Found 189 matching spots\n",
      "  Chunk 97: Found 205 matching spots\n",
      "  Chunk 98: Found 279 matching spots\n",
      "  Chunk 99: Found 216 matching spots\n",
      "  Chunk 100: Found 169 matching spots\n",
      "  Processed 50000 rows...\n",
      "  Chunk 101: Found 242 matching spots\n",
      "  Chunk 102: Found 285 matching spots\n",
      "  Chunk 103: Found 276 matching spots\n",
      "  Chunk 104: Found 378 matching spots\n",
      "  Chunk 105: Found 375 matching spots\n",
      "  Chunk 106: Found 191 matching spots\n",
      "  Chunk 107: Found 169 matching spots\n",
      "  Chunk 108: Found 175 matching spots\n",
      "  Chunk 109: Found 196 matching spots\n",
      "  Chunk 110: Found 172 matching spots\n",
      "  Processed 55000 rows...\n",
      "  Chunk 111: Found 176 matching spots\n",
      "  Chunk 112: Found 280 matching spots\n",
      "  Chunk 113: Found 333 matching spots\n",
      "  Chunk 114: Found 274 matching spots\n",
      "  Chunk 115: Found 177 matching spots\n",
      "  Chunk 116: Found 228 matching spots\n",
      "  Chunk 117: Found 384 matching spots\n",
      "  Chunk 118: Found 419 matching spots\n",
      "  Chunk 119: Found 314 matching spots\n",
      "  Chunk 120: Found 226 matching spots\n",
      "  Processed 60000 rows...\n",
      "  Chunk 121: Found 127 matching spots\n",
      "  Chunk 122: Found 226 matching spots\n",
      "  Chunk 123: Found 352 matching spots\n",
      "  Chunk 124: Found 321 matching spots\n",
      "  Chunk 125: Found 417 matching spots\n",
      "  Chunk 126: Found 286 matching spots\n",
      "  Chunk 127: Found 281 matching spots\n",
      "  Chunk 128: Found 185 matching spots\n",
      "  Chunk 129: Found 152 matching spots\n",
      "  Chunk 130: Found 88 matching spots\n",
      "  Processed 65000 rows...\n",
      "  Chunk 131: Found 112 matching spots\n",
      "  Chunk 132: Found 131 matching spots\n",
      "  Chunk 133: Found 315 matching spots\n",
      "  Chunk 134: Found 182 matching spots\n",
      "  Chunk 135: Found 276 matching spots\n",
      "  Chunk 136: Found 237 matching spots\n",
      "  Chunk 137: Found 297 matching spots\n",
      "  Chunk 138: Found 424 matching spots\n",
      "  Chunk 139: Found 107 matching spots\n",
      "  Chunk 140: Found 143 matching spots\n",
      "  Processed 70000 rows...\n",
      "  Chunk 141: Found 107 matching spots\n",
      "  Chunk 142: Found 222 matching spots\n",
      "  Chunk 143: Found 245 matching spots\n",
      "  Chunk 144: Found 279 matching spots\n",
      "  Chunk 145: Found 397 matching spots\n",
      "  Chunk 146: Found 207 matching spots\n",
      "  Chunk 147: Found 229 matching spots\n",
      "  Chunk 148: Found 86 matching spots\n",
      "  Chunk 149: Found 31 matching spots\n",
      "  Chunk 150: Found 137 matching spots\n",
      "  Processed 75000 rows...\n",
      "  Chunk 151: Found 361 matching spots\n",
      "  Chunk 152: Found 258 matching spots\n",
      "  Chunk 153: Found 125 matching spots\n",
      "  Chunk 154: Found 296 matching spots\n",
      "  Chunk 155: Found 240 matching spots\n",
      "  Chunk 156: Found 270 matching spots\n",
      "  Chunk 157: Found 161 matching spots\n",
      "  Chunk 158: Found 167 matching spots\n",
      "  Chunk 159: Found 164 matching spots\n",
      "  Chunk 160: Found 126 matching spots\n",
      "  Processed 80000 rows...\n",
      "  Chunk 161: Found 234 matching spots\n",
      "  Chunk 162: Found 314 matching spots\n",
      "  Chunk 163: Found 285 matching spots\n",
      "  Chunk 164: Found 97 matching spots\n",
      "  Chunk 165: Found 87 matching spots\n",
      "  Chunk 166: Found 152 matching spots\n",
      "  Chunk 167: Found 119 matching spots\n",
      "  Chunk 168: Found 128 matching spots\n",
      "  Chunk 169: Found 161 matching spots\n",
      "  Chunk 170: Found 239 matching spots\n",
      "  Processed 85000 rows...\n",
      "  Chunk 171: Found 315 matching spots\n",
      "  Chunk 172: Found 337 matching spots\n",
      "  Chunk 173: Found 180 matching spots\n",
      "  Chunk 174: Found 308 matching spots\n",
      "  Chunk 175: Found 347 matching spots\n",
      "  Chunk 176: Found 305 matching spots\n",
      "  Chunk 177: Found 398 matching spots\n",
      "  Chunk 178: Found 387 matching spots\n",
      "  Chunk 179: Found 310 matching spots\n",
      "  Chunk 180: Found 274 matching spots\n",
      "  Processed 90000 rows...\n",
      "  Chunk 181: Found 218 matching spots\n",
      "  Chunk 182: Found 205 matching spots\n",
      "  Chunk 183: Found 192 matching spots\n",
      "  Chunk 184: Found 258 matching spots\n",
      "  Chunk 185: Found 202 matching spots\n",
      "  Chunk 186: Found 302 matching spots\n",
      "  Chunk 187: Found 215 matching spots\n",
      "  Chunk 188: Found 126 matching spots\n",
      "  Chunk 189: Found 208 matching spots\n",
      "  Chunk 190: Found 350 matching spots\n",
      "  Processed 95000 rows...\n",
      "  Chunk 191: Found 369 matching spots\n",
      "  Chunk 192: Found 296 matching spots\n",
      "  Chunk 193: Found 197 matching spots\n",
      "  Chunk 194: Found 309 matching spots\n",
      "  Chunk 195: Found 309 matching spots\n",
      "  Chunk 196: Found 363 matching spots\n",
      "  Chunk 197: Found 417 matching spots\n",
      "  Chunk 198: Found 269 matching spots\n",
      "✅ Successfully loaded 48172 spots from 198 chunks\n",
      "Tissue distribution in result:\n",
      "  connective tissue: 30330 spots\n",
      "  invasive cancer: 12106 spots\n",
      "  adipose tissue: 3716 spots\n",
      "  immune infiltrate: 1873 spots\n",
      "  cancer in situ: 96 spots\n",
      "  breast glands: 51 spots\n",
      "\n",
      "Multi-tissue result shape: (48172, 14643)\n",
      "Sample spot names: ['TNBC1_spot2x18', 'TNBC1_spot2x20', 'TNBC1_spot2x22']\n"
     ]
    }
   ],
   "source": [
    "# Test: Load multiple tissue types at once\n",
    "print(\"\\n=== LOADING MULTIPLE TISSUE TYPES ===\")\n",
    "\n",
    "# Check what tissue types are available\n",
    "print(\"Available tissue types:\")\n",
    "tissue_counts = annotation['label'].value_counts()\n",
    "print(tissue_counts)\n",
    "\n",
    "# Load multiple tissue types (example: adipose + connective tissue)\n",
    "multiple_tissues = list(annotation[annotation['label'] != 'undetermined']['label'].unique())\n",
    "multi_tissue_spots = load_multiple_tissues(file_path, annotation, multiple_tissues, chunk_size=500)\n",
    "\n",
    "print(f\"\\nMulti-tissue result shape: {multi_tissue_spots.shape}\")\n",
    "if len(multi_tissue_spots) > 0:\n",
    "    print(\"Sample spot names:\", multi_tissue_spots.index[:3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce8bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tissue_spots.to_parquet(\"/storage/research/dbmr_luisierlab/temp/lfournier/repositories/TumorArchetype-FM/results/molecular/TNBC_processed/TNBC_filtered_counts_annotated_spots.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512cb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet file\n",
    "normalized_counts = \"/storage/research/dbmr_luisierlab/temp/lfournier/repositories/TumorArchetype-FM/results/molecular/TNBC_processed/TNBC_filtered_normalized_counts_annotated_spots.parquet\"\n",
    "filtered_normalized = pd.read_parquet(normalized_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b78c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>AAMDC</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AAR2</th>\n",
       "      <th>...</th>\n",
       "      <th>LHX2</th>\n",
       "      <th>C12orf56</th>\n",
       "      <th>GATA5</th>\n",
       "      <th>HECW1</th>\n",
       "      <th>KRT6C</th>\n",
       "      <th>PRKG2</th>\n",
       "      <th>CLEC4D</th>\n",
       "      <th>HSPB9</th>\n",
       "      <th>UTS2</th>\n",
       "      <th>MTMR8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TNBC1_spot2x18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.371040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.928458</td>\n",
       "      <td>2.284252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC1_spot2x20</th>\n",
       "      <td>1.740722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.740722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC1_spot2x22</th>\n",
       "      <td>1.578796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.578796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC1_spot2x24</th>\n",
       "      <td>1.589578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082089</td>\n",
       "      <td>1.082089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC1_spot2x26</th>\n",
       "      <td>2.371941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870586</td>\n",
       "      <td>1.328830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC96_spot64x22</th>\n",
       "      <td>2.977004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC96_spot64x24</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.466337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.466337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC96_spot64x26</th>\n",
       "      <td>2.958109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.537362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC96_spot64x28</th>\n",
       "      <td>2.826836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNBC96_spot64x30</th>\n",
       "      <td>1.548934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.548934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.548934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48172 rows × 14643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       A2M  A2ML1  A4GALT      AAAS  AACS  AAGAB      AAK1  \\\n",
       "spot_id                                                                      \n",
       "TNBC1_spot2x18    0.000000    0.0     0.0  0.000000   0.0    0.0  1.371040   \n",
       "TNBC1_spot2x20    1.740722    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC1_spot2x22    1.578796    0.0     0.0  0.000000   0.0    0.0  1.578796   \n",
       "TNBC1_spot2x24    1.589578    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC1_spot2x26    2.371941    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "...                    ...    ...     ...       ...   ...    ...       ...   \n",
       "TNBC96_spot64x22  2.977004    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC96_spot64x24  3.044522    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC96_spot64x26  2.958109    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC96_spot64x28  2.826836    0.0     0.0  0.000000   0.0    0.0  0.000000   \n",
       "TNBC96_spot64x30  1.548934    0.0     0.0  1.548934   0.0    0.0  0.000000   \n",
       "\n",
       "                     AAMDC      AAMP      AAR2  ...  LHX2  C12orf56  GATA5  \\\n",
       "spot_id                                         ...                          \n",
       "TNBC1_spot2x18    0.000000  1.928458  2.284252  ...   0.0       0.0    0.0   \n",
       "TNBC1_spot2x20    0.000000  0.000000  1.740722  ...   0.0       0.0    0.0   \n",
       "TNBC1_spot2x22    0.000000  0.000000  0.000000  ...   0.0       0.0    0.0   \n",
       "TNBC1_spot2x24    0.000000  1.082089  1.082089  ...   0.0       0.0    0.0   \n",
       "TNBC1_spot2x26    0.870586  1.328830  0.000000  ...   0.0       0.0    0.0   \n",
       "...                    ...       ...       ...  ...   ...       ...    ...   \n",
       "TNBC96_spot64x22  0.000000  0.000000  0.000000  ...   0.0       0.0    0.0   \n",
       "TNBC96_spot64x24  1.466337  0.000000  1.466337  ...   0.0       0.0    0.0   \n",
       "TNBC96_spot64x26  0.000000  0.000000  1.537362  ...   0.0       0.0    0.0   \n",
       "TNBC96_spot64x28  0.000000  0.000000  0.000000  ...   0.0       0.0    0.0   \n",
       "TNBC96_spot64x30  0.000000  1.548934  0.000000  ...   0.0       0.0    0.0   \n",
       "\n",
       "                  HECW1  KRT6C  PRKG2  CLEC4D  HSPB9  UTS2  MTMR8  \n",
       "spot_id                                                            \n",
       "TNBC1_spot2x18      0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC1_spot2x20      0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC1_spot2x22      0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC1_spot2x24      0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC1_spot2x26      0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "...                 ...    ...    ...     ...    ...   ...    ...  \n",
       "TNBC96_spot64x22    0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC96_spot64x24    0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC96_spot64x26    0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC96_spot64x28    0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "TNBC96_spot64x30    0.0    0.0    0.0     0.0    0.0   0.0    0.0  \n",
       "\n",
       "[48172 rows x 14643 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031b6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalhisto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
